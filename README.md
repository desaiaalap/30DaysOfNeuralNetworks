
![Progress](https://img.shields.io/badge/Progress-0%25-red)


# 30 Days of Neural Networks ğŸš€

## Overview
This repository documents my 30-day journey of learning, implementing, and experimenting with neural networks. The challenge is designed to progressively build my knowledge and skills in deep learning, covering foundational concepts, advanced architectures, and real-world applications.

---

## ğŸ“… Challenge Structure

### **Week 1: Foundations of Neural Networks**
- **Goal**: Understand the basics of perceptrons, sigmoid neurons, network architecture, and gradient descent.
- **Daily Focus**:
  - Day 1: Learn about perceptrons and implement logic gates.
  - Day 2: Explore sigmoid neurons and compare them to perceptrons.
  - Day 3: Study feedforward propagation and network design.
  - Day 4: Understand gradient descent with a simple cost function.
  - Day 5: Build a basic MNIST digit classifier.
  - Day 6: Evaluate the MNIST classifier with metrics.
  - Day 7: Reflect on Week 1 progress and consolidate learning.

### **Week 2: Backpropagation and Learning Improvements**
- **Goal**: Dive into backpropagation, cost functions, and regularization techniques.
- **Daily Focus**:
  - Day 8: Implement backpropagation in the MNIST classifier.
  - Day 9: Introduce cross-entropy as the cost function.
  - Day 10: Explore L2 regularization to prevent overfitting.
  - Day 11: Learn about weight initialization techniques.
  - Day 12: Retrain the classifier with multiple improvements.
  - Day 13: Visualize learning progress and evaluate performance.
  - Day 14: Reflect on Week 2 advancements.

### **Week 3: Advanced Neural Network Architectures**
- **Goal**: Understand the universality of neural networks, CNNs, and techniques to overcome vanishing gradients.
- **Daily Focus**:
  - Day 15: Demonstrate universality by approximating mathematical functions.
  - Day 16: Build a convolutional neural network (CNN) for image classification.
  - Day 17: Use dropout for regularization in CNNs.
  - Day 18: Address the vanishing gradient problem using ReLU activation.
  - Day 19: Fine-tune a pre-trained model with transfer learning.
  - Day 20: Compare multiple CNN architectures.
  - Day 21: Reflect on Week 3 and future opportunities.

### **Week 4: Real-World Applications**
- **Goal**: Apply neural networks to GANs, time-series analysis, and model interpretability.
- **Daily Focus**:
  - Day 22: Learn about GANs and train one to generate synthetic data.
  - Day 23: Implement time-series analysis using LSTMs.
  - Day 24: Use SHAP for neural network interpretability.
  - Day 25: Analyze cricket performance using a neural network.
  - Day 26: Perform hyperparameter tuning for improved results.
  - Days 27â€“28: Create a capstone project and report.
  - Days 29â€“30: Reflect on the entire 30-day journey.

---

## ğŸ“‚ Repository Structure
```plaintext
30DaysOfNeuralNetworks/
  Week1/
    Day1/
      perceptrons_logic_gates.ipynb
      blog_draft.md
    Day2/
      sigmoid_neurons.ipynb
      blog_draft.md
    ...
  Week4/
    ...
  README.md
```

---

## ğŸ› ï¸ Tools and Technologies
- **Languages**: Python
- **Libraries**: NumPy, Pandas, Matplotlib, TensorFlow, Keras
- **Environment**: Jupyter Notebook, Google Colab

---

## âœ… Progress Tracker
| Day | Topic                                      | Status   |
|-----|-------------------------------------------|----------|
| 1   | Perceptrons and Logic Gates               | âœ… Done  |
| 2   | Sigmoid Neurons                           | ğŸ”² Pending   |
| 3   | Feedforward Network Design               | ğŸ”² Pending|
| 4   | Gradient Descent                          | ğŸ”² Pending|
| 5   | MNIST Digit Classifier                    | ğŸ”² Pending|
| 6   | Evaluating MNIST Classifier              | ğŸ”² Pending|
| 7   | Reflection on Week 1                     | ğŸ”² Pending|
| 8   | Backpropagation Implementation            | ğŸ”² Pending|
| 9   | Cross-Entropy Cost Function              | ğŸ”² Pending|
| 10  | L2 Regularization                         | ğŸ”² Pending|
| 11  | Weight Initialization                    | ğŸ”² Pending|
| 12  | Retrain with Improvements                | ğŸ”² Pending|
| 13  | Visualize Learning Progress              | ğŸ”² Pending|
| 14  | Reflection on Week 2                     | ğŸ”² Pending|
| 15  | Universality of Neural Networks           | ğŸ”² Pending|
| 16  | Build a CNN for Image Classification     | ğŸ”² Pending|
| 17  | Dropout for Regularization               | ğŸ”² Pending|
| 18  | ReLU Activation for Vanishing Gradients  | ğŸ”² Pending|
| 19  | Transfer Learning                        | ğŸ”² Pending|
| 20  | Compare CNN Architectures                | ğŸ”² Pending|
| 21  | Reflection on Week 3                     | ğŸ”² Pending|
| 22  | GANs and Synthetic Data Generation       | ğŸ”² Pending|
| 23  | Time-Series Analysis with LSTMs          | ğŸ”² Pending|
| 24  | SHAP for Neural Network Interpretability | ğŸ”² Pending|
| 25  | Cricket Performance Analysis             | ğŸ”² Pending|
| 26  | Hyperparameter Tuning                    | ğŸ”² Pending|
| 27  | Capstone Project - Part 1                | ğŸ”² Pending|
| 28  | Capstone Project - Part 2                | ğŸ”² Pending|
| 29  | Reflection on the Journey - Part 1       | ğŸ”² Pending|
| 30  | Reflection on the Journey - Part 2       | ğŸ”² Pending|

---

Feel free to clone, explore, and contribute! Let's build neural networks, one day at a time! ğŸ˜Š
